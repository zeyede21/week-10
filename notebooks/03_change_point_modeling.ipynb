{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8007066d-61e0-4147-b024-292f7a6b0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "src_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c131ec70-604e-4a3f-a449-adf1752e71c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mruptures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrpt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01marviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maz\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymc'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ruptures as rpt\n",
    "import pymc as pm\n",
    "import arviz as az "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcb4d4-0d5c-4c38-b876-ffaa6c698c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cd293-4d66-418a-884a-43acf886f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc5e03-151e-4155-a314-c34cf635329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import aesara.tensor as at\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.randn(100)\n",
    "n = len(data)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n)\n",
    "    mu1 = pm.Normal(\"mu1\", mu=0, sigma=1)\n",
    "    mu2 = pm.Normal(\"mu2\", mu=0, sigma=1)\n",
    "    \n",
    "    mu = pm.math.switch(tau >= at.arange(n), mu1, mu2)\n",
    "    obs = pm.Normal(\"obs\", mu=mu, sigma=1, observed=data)\n",
    "\n",
    "    trace = pm.sample(\n",
    "    draws=2000,\n",
    "    tune=2000,\n",
    "    target_accept=0.98,  # very cautious step size\n",
    "    return_inferencedata=True\n",
    "             )\n",
    "\n",
    "    pm.summary(trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f0de0a-f045-47c9-a88e-3d8e8351cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.summary(trace, hdi_prob=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76939eed-c497-465e-a4d5-847872485d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace)\n",
    "plt.show()\n",
    "\n",
    "az.plot_posterior(trace)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc233a5c-c340-48f0-9f4b-aeaa7e19e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_samples = trace.posterior['tau'].values.flatten()\n",
    "estimated_tau = int(np.mean(tau_samples))\n",
    "print(\"Estimated change point (index):\", estimated_tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed0ccd-69db-44fa-9811-71134abdda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed/brent_clean.csv\"\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    # Use format='mixed' to infer the format for each date string\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='mixed', dayfirst=False)\n",
    "    data = data.sort_values(by='Date')\n",
    "    data = data.set_index('Date')\n",
    "\n",
    "    # Handle missing values by interpolation, as suggested for robust analysis\n",
    "    # This is crucial for time series data to maintain continuity\n",
    "    data['Price'] = data['Price'].interpolate(method='time')\n",
    "\n",
    "    # Drop any rows where 'Price' might still be NaN after interpolation (e.g., at the very beginning/end if no surrounding data)\n",
    "    data = data.dropna(subset=['Price'])\n",
    "\n",
    "    # For Bayesian analysis of volatility, it's often better to work with log returns\n",
    "    # Log returns help in achieving stationarity and are directly related to volatility.\n",
    "    # We use.diff() and.dropna() to handle the first NaN value from the difference calculation.\n",
    "    data['Log_Return'] = np.log(data['Price']).diff().dropna()\n",
    "\n",
    "    # The time series for analysis will be the 'Price' for ruptures and 'Log_Return' for PyMC\n",
    "    time_series_price = data['Price']\n",
    "    time_series_log_return = data['Log_Return'].dropna() # Ensure log returns are also dropped if NaN\n",
    "\n",
    "    print(\"Brent Oil Prices Time Series Head:\")\n",
    "    display(time_series_price.head())\n",
    "    print(\"\\nBrent Oil Prices Time Series Tail:\")\n",
    "    display(time_series_price.tail())\n",
    "    print(\"\\nBrent Oil Log Returns Time Series Head:\")\n",
    "    display(time_series_log_return.head())\n",
    "    print(\"\\nBrent Oil Log Returns Time Series Tail:\")\n",
    "    display(time_series_log_return.tail())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{path}' was not found. Please update the 'path' variable with the correct file location.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading or preprocessing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eea027-8211-4de4-b2cd-ea05152c3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50863ef3-89dd-4aa4-a17e-51895fbba104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['Date', 'daily_return']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775613f-d315-48cf-bd8f-3e7af9310021",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['daily_return'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6468be0-64d7-4159-a198-a8519cd76803",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03f45c-0d39-4e9e-b085-e035b4ceaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['Date'], df['daily_return'], label='Daily Return')\n",
    "plt.axvline(x=df['Date'].iloc[estimated_tau], color='red', linestyle='--', label='Estimated Change Point')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Return')\n",
    "plt.title('Change Point Detection')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('reports/figures', exist_ok=True)\n",
    "\n",
    "# Then save the figure\n",
    "plt.savefig('reports/figures/change_point_plot.png')\n",
    "\n",
    "plt.savefig('reports/figures/change_point_plot.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ebe26-e05c-4fb9-951d-32e2562cefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time series to a numpy array for the ruptures library\n",
    "# We use the raw price data here to detect shifts in the mean price level.\n",
    "if 'time_series_price' in locals() and not time_series_price.empty: # Check if time_series_price was successfully created and is not empty\n",
    "    signal_array_ruptures = time_series_price.values.reshape(-1, 1) # Reshape for ruptures\n",
    "\n",
    "    # Define the model and algorithm\n",
    "    # 'l2' (L2 norm) cost function is good for detecting changes in mean.\n",
    "    model_ruptures = \"l2\"\n",
    "    algo_ruptures = rpt.Pelt(model=model_ruptures).fit(signal_array_ruptures)\n",
    "\n",
    "    # Predict change points\n",
    "    # The 'pen' (penalty) parameter controls the number of detected change points.\n",
    "    # A higher penalty results in fewer change points.\n",
    "    # You might need to tune this penalty for your specific dataset.\n",
    "    # A common heuristic is 3 * log(n_samples) or 4 * log(n_samples).\n",
    "\n",
    "    penalty_value_ruptures = 3 * np.log(len(time_series_price))\n",
    "    change_points_ruptures = algo_ruptures.predict(pen=penalty_value_ruptures)\n",
    "\n",
    "    # The 'ruptures' library returns the index of the last point of each segment.\n",
    "    # We usually exclude the last point if it's just the end of the series.\n",
    "    if change_points_ruptures and change_points_ruptures[-1] == len(time_series_price):\n",
    "        detected_change_point_indices_ruptures = change_points_ruptures[:-1]\n",
    "    else:\n",
    "        detected_change_point_indices_ruptures = change_points_ruptures\n",
    "\n",
    "    print(f\"\\nDetected Change Point Indices (ruptures on Price): {detected_change_point_indices_ruptures}\")\n",
    "\n",
    "    # Convert indices to dates for plotting\n",
    "    detected_change_point_dates_ruptures = time_series_price.index[detected_change_point_indices_ruptures]\n",
    "    print(f\"Detected Change Point Dates (ruptures on Price): {detected_change_point_dates_ruptures.tolist()}\")\n",
    "\n",
    "    # Visualization for ruptures\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.plot(time_series_price.index, time_series_price.values, label='Brent Oil Price', color='blue', alpha=0.7)\n",
    "    # Plot change points with labels, ensuring only one label is created for the legend\n",
    "    for i, cp_date in enumerate(detected_change_point_dates_ruptures):\n",
    "        plt.axvline(x=cp_date, color='red', linestyle='--', linewidth=2, label='Detected Change Point (ruptures)' if i == 0 else \"\")\n",
    "    plt.title('Frequentist Change Point Detection (ruptures - PELT) on Brent Oil Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping Frequentist Change Point Detection as price time series data is not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58478776-041d-4be0-9965-c72f3f50b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Bayesian methods, we often define a probabilistic model.\n",
    "# Let's model a single change point where the *volatility* (standard deviation) changes.\n",
    "# This is applied to the log returns, as volatility is more directly observed there.\n",
    "\n",
    "if 'time_series_log_return' in locals() and not time_series_log_return.empty: # Check if log returns were successfully created and are not empty\n",
    "    # Convert log returns to numpy array for PyMC\n",
    "    data_pymc_log_return = time_series_log_return.values\n",
    "\n",
    "    # Define the Bayesian model\n",
    "    with pm.Model() as bayesian_volatility_change_point_model:\n",
    "        # Prior for the change point location (uniform over the time series)\n",
    "        # We choose a discrete uniform distribution for the index of the change point.\n",
    "        tau = pm.DiscreteUniform(\"tau\", lower=0, upper=len(data_pymc_log_return) - 1)\n",
    "\n",
    "        # Priors for the standard deviations (volatility) before and after the change point\n",
    "        # We use HalfNormal distributions for standard deviations, as they must be positive.\n",
    "        # Adjust sigma values based on expected scale of log returns.\n",
    "        sigma_1 = pm.HalfNormal(\"sigma_1\", sigma=0.1) # Volatility before change\n",
    "        sigma_2 = pm.HalfNormal(\"sigma_2\", sigma=0.1) # Volatility after change\n",
    "\n",
    "        # Assume the mean of log returns is constant (e.g., around zero for daily returns)\n",
    "        # or you could model it as well if you expect mean shifts in returns.\n",
    "        # For simplicity, let's assume a constant mean for log returns.\n",
    "        # A small normal prior around 0 is often reasonable for log returns.\n",
    "        mu_log_return = pm.Normal(\"mu_log_return\", mu=0, sigma=0.01)\n",
    "\n",
    "        # Define the standard deviation for each data point based on the change point location (tau)\n",
    "        idx = np.arange(len(data_pymc_log_return))\n",
    "        sigma = pm.math.switch(idx < tau, sigma_1, sigma_2)\n",
    "\n",
    "        # Likelihood of the observed data (log returns)\n",
    "        # The observed log returns are assumed to be normally distributed around `mu_log_return` with standard deviation `sigma`.\n",
    "        observation = pm.Normal(\"observation\", mu=mu_log_return, sigma=sigma, observed=data_pymc_log_return)\n",
    "\n",
    "        # Sample from the posterior distribution using NUTS (No-U-Turn Sampler)\n",
    "        # This is where the MCMC magic happens.\n",
    "        # `draws`: number of samples from the posterior.\n",
    "        # `tune`: number of tuning (warmup) steps.\n",
    "        # `chains`: number of independent sampling chains.\n",
    "        # `cores`: number of CPU cores to use for sampling (set to 1 for simpler debugging, or higher for speed).\n",
    "        # Sampling can take a while for large datasets.\n",
    "        print(\"\\nStarting PyMC sampling for Bayesian Volatility Change Point Detection...\")\n",
    "        trace_volatility = pm.sample(draws=4000, tune=3000, chains=4, random_seed=42, return_inferencedata=True, cores=2)\n",
    "        print(\"PyMC Sampling Complete.\")\n",
    "\n",
    "    print(\"\\nBayesian Volatility Model Sampling Complete. Summary of parameters:\")\n",
    "    display(az.summary(trace_volatility, var_names=[\"tau\", \"mu_log_return\", \"sigma_1\", \"sigma_2\"]))\n",
    "\n",
    "    # Analyze the posterior distribution of the change point (tau)\n",
    "    tau_samples_volatility = trace_volatility.posterior[\"tau\"].values.flatten()\n",
    "\n",
    "    # Calculate the most probable change point index (mode of the posterior)\n",
    "    # Note: PyMC's tau is an index relative to the `data_pymc_log_return` array, which starts after the first log return.\n",
    "    # So, we need to adjust the index for the original `time_series_log_return` index.\n",
    "    most_probable_tau_index_volatility = int(pd.Series(tau_samples_volatility).mode().iloc[0])\n",
    "    # The log_return series starts one day after the price series, so adjust index for original date mapping\n",
    "    most_probable_tau_date_volatility = time_series_log_return.index[most_probable_tau_index_volatility]\n",
    "\n",
    "    print(f\"\\nMost Probable Change Point Index (PyMC on Log Returns): {most_probable_tau_index_volatility}\")\n",
    "    print(f\"Most Probable Change Point Date (PyMC on Log Returns): {most_probable_tau_date_volatility}\")\n",
    "\n",
    "    # Visualization for PyMC results\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.plot(time_series_log_return.index, time_series_log_return.values, label='Brent Oil Log Returns', color='blue', alpha=0.7)\n",
    "\n",
    "    # Plot the most probable change point\n",
    "    plt.axvline(x=most_probable_tau_date_volatility, color='green', linestyle='-', linewidth=2, label='Most Probable Volatility Change Point (PyMC)')\n",
    "\n",
    "    # Optionally, plot the posterior distribution of the change point\n",
    "    # This shows the uncertainty around the change point location.\n",
    "    # We map the indices back to dates for the histogram.\n",
    "    plt.hist(time_series_log_return.index[tau_samples_volatility], bins=50, density=True, alpha=0.3, color='orange', label='Posterior Probability of Volatility Change Point')\n",
    "\n",
    "    plt.title('Bayesian Change Point Detection (PyMC) on Brent Oil Log Returns (Volatility Shift)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Log Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping Bayesian Change Point Detection as log return data is not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4f4fe-d6e9-47c9-a170-ee4ec0a54aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume these are already loaded or computed:\n",
    "# time_series_price: pd.Series indexed by date with Brent oil prices\n",
    "# detected_change_point_indices_ruptures: list of rupture change point indices (frequentist)\n",
    "# trace_volatility: arviz.InferenceData from PyMC3 Bayesian model\n",
    "# time_series_log_return: pd.Series indexed by date with log returns\n",
    "\n",
    "# ------------------------\n",
    "# 1. Plot Price with Rupture Change Points\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(time_series_price.index, time_series_price.values, label='Brent Oil Price')\n",
    "for cp in detected_change_point_indices_ruptures:\n",
    "    plt.axvline(time_series_price.index[cp], color='red', linestyle='--', alpha=0.7, label='Rupture Change Point' if cp == detected_change_point_indices_ruptures[0] else \"\")\n",
    "plt.title('Brent Oil Price with Detected Change Points (Ruptures)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------\n",
    "# 2. Plot Bayesian Posterior Distribution for tau (Change Point)\n",
    "tau_samples = trace_volatility.posterior[\"tau\"].values.flatten()\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.hist(time_series_log_return.index[tau_samples.astype(int)], bins=50, density=True, color='orange', alpha=0.5)\n",
    "plt.title('Posterior Distribution of Change Point (Bayesian Model)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------\n",
    "# 3. Quantify Impact for Ruptures (Price Mean Shift)\n",
    "\n",
    "if detected_change_point_indices_ruptures and len(detected_change_point_indices_ruptures) > 0:\n",
    "    print(\"\\n--- Ruptures (Price Mean Shift) Impact ---\")\n",
    "    segments = [0] + detected_change_point_indices_ruptures + [len(time_series_price)]\n",
    "    for i in range(len(segments)-1):\n",
    "        start_idx = segments[i]\n",
    "        end_idx = segments[i+1]\n",
    "        segment_data = time_series_price.iloc[start_idx:end_idx]\n",
    "        if not segment_data.empty:\n",
    "            end_date_idx = end_idx - 1 if end_idx > 0 else 0\n",
    "            print(f\"Segment {i+1} (Price): {time_series_price.index[start_idx].strftime('%Y-%m-%d')} to {time_series_price.index[end_date_idx].strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Mean Price: {segment_data.mean():.2f} USD\")\n",
    "            print(f\"  Std Dev Price: {segment_data.std():.2f} USD\")\n",
    "            if i > 0:\n",
    "                prev_segment_data = time_series_price.iloc[segments[i-1]:segments[i]]\n",
    "                if not prev_segment_data.empty:\n",
    "                    prev_segment_mean = prev_segment_data.mean()\n",
    "                    price_change = segment_data.mean() - prev_segment_mean\n",
    "                    percent_change = (price_change / prev_segment_mean) * 100 if prev_segment_mean != 0 else float('inf')\n",
    "                    print(f\"  Change from previous segment: {price_change:.2f} USD ({percent_change:.2f}%)\")\n",
    "else:\n",
    "    print(\"No rupture change points detected or data unavailable.\")\n",
    "\n",
    "# ------------------------\n",
    "# 4. Quantify Impact for PyMC Bayesian Volatility Shift\n",
    "\n",
    "print(\"\\n--- Bayesian Volatility Shift Impact ---\")\n",
    "posterior_mu_log_return = trace_volatility.posterior[\"mu_log_return\"].mean().item()\n",
    "posterior_sigma_1 = trace_volatility.posterior[\"sigma_1\"].mean().item()\n",
    "posterior_sigma_2 = trace_volatility.posterior[\"sigma_2\"].mean().item()\n",
    "\n",
    "print(f\"Estimated Mean Log Return (overall): {posterior_mu_log_return:.4f}\")\n",
    "print(f\"Estimated Std Dev Before Change (sigma_1): {posterior_sigma_1:.4f}\")\n",
    "print(f\"Estimated Std Dev After Change (sigma_2): {posterior_sigma_2:.4f}\")\n",
    "\n",
    "percent_change_volatility = ((posterior_sigma_2 - posterior_sigma_1) / posterior_sigma_1) * 100 if posterior_sigma_1 != 0 else float('inf')\n",
    "print(f\"Percentage change in volatility: {percent_change_volatility:.2f}%\")\n",
    "\n",
    "most_probable_tau_index = int(pd.Series(tau_samples).mode())\n",
    "most_probable_tau_date = time_series_log_return.index[most_probable_tau_index]\n",
    "print(f\"Most Probable Change Point Index (Bayesian tau): {most_probable_tau_index}\")\n",
    "print(f\"Most Probable Change Point Date (Bayesian tau): {most_probable_tau_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# ------------------------\n",
    "# 5. Hypothesize Real-World Event Associations\n",
    "\n",
    "print(\"\\n--- Hypothesized Event Associations ---\")\n",
    "# Example: You should replace these dates and events with your Task 1 researched events\n",
    "events_dict = {\n",
    "    '1987-08-20': 'OPEC Production Cut Announcement',\n",
    "    '1990-08-06': 'Gulf War Start',\n",
    "    '2008-08-20': 'Global Financial Crisis Peaks',\n",
    "    'Most probable Bayesian change point': most_probable_tau_date.strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "for date_str, event_desc in events_dict.items():\n",
    "    print(f\"Date: {date_str}, Event: {event_desc}\")\n",
    "\n",
    "# ------------------------\n",
    "# 6. Convergence Diagnostics - Trace Plots and R-hat values\n",
    "\n",
    "print(\"\\n--- Convergence Diagnostics ---\")\n",
    "summary = az.summary(trace_volatility, var_names=[\"tau\", \"mu_log_return\", \"sigma_1\", \"sigma_2\"])\n",
    "print(summary[['mean','sd','r_hat','ess_bulk']])\n",
    "\n",
    "az.plot_trace(trace_volatility, var_names=[\"tau\", \"mu_log_return\", \"sigma_1\", \"sigma_2\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f678761-c70e-4477-b98c-f3a38f0bd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot trace for model diagnostics\n",
    "az.plot_trace(trace_volatility, var_names=[\"tau\", \"mu_log_return\", \"sigma_1\", \"sigma_2\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot posterior distribution for change point\n",
    "az.plot_posterior(trace_volatility, var_names=[\"tau\"], hdi_prob=0.95)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot mean and std dev segments side by side\n",
    "segments = [0] + detected_change_point_indices_ruptures + [len(time_series_price)]\n",
    "means = []\n",
    "stds = []\n",
    "for i in range(len(segments)-1):\n",
    "    segment_data = time_series_price.iloc[segments[i]:segments[i+1]]\n",
    "    means.append(segment_data.mean())\n",
    "    stds.append(segment_data.std())\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.errorbar(range(len(means)), means, yerr=stds, fmt='o', capsize=5)\n",
    "plt.xlabel('Segment')\n",
    "plt.ylabel('Mean Price with Std Dev')\n",
    "plt.title('Segment-wise Mean Price and Volatility')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938c6f2-1dca-4ce7-a4fe-24669738117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Assuming your trace object is named 'trace_volatility'\n",
    "summary_stats = az.summary(trace_volatility)\n",
    "print(summary_stats[['r_hat']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c2cac-74bc-4c78-be3f-8aab309675d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_trace(trace_volatility, var_names=[\"tau\", \"mu_log_return\", \"sigma_1\", \"sigma_2\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98687ec-9048-4f8d-b694-c801c4f83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Conceptual Further Analysis ---\")\n",
    "print(\"For both frequentist and Bayesian approaches, the next steps would typically involve:\")\n",
    "print(\"a. Contextualizing: What real-world events or policy changes occurred around these dates?\")\n",
    "print(\"   - For example, if analyzing Brent oil prices, a change point might correspond to a major OPEC decision, a geopolitical conflict, or a global economic sanction.\")\n",
    "print(\"b. Quantifying Impact: Analyze the statistical properties (mean, variance, trend) of the segments\")\n",
    "print(\"   before and after each change point to understand the magnitude and nature of the shift.\")\n",
    "\n",
    "# Example of quantifying impact for ruptures (Price Mean Shift)\n",
    "if 'detected_change_point_indices_ruptures' in locals() and len(detected_change_point_indices_ruptures) > 0 and 'time_series_price' in locals():\n",
    "    print(\"\\n--- Ruptures (Price Mean Shift) Impact ---\")\n",
    "    segments = [0] + detected_change_point_indices_ruptures + [len(time_series_price)]\n",
    "    for i in range(len(segments) - 1):\n",
    "        start_idx = segments[i]\n",
    "        end_idx = segments[i+1]\n",
    "        segment_data = time_series_price.iloc[start_idx:end_idx]\n",
    "        if not segment_data.empty:\n",
    "            # Ensure end_idx is within bounds when accessing index\n",
    "            end_date_idx = end_idx - 1 if end_idx > 0 else 0\n",
    "            print(f\"Segment {i+1} (Price): {time_series_price.index[start_idx].strftime('%Y-%m-%d')} to {time_series_price.index[end_date_idx].strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Mean Price: {segment_data.mean():.2f} USD\")\n",
    "            print(f\"  Std Dev Price: {segment_data.std():.2f} USD\")\n",
    "            if i > 0:\n",
    "                prev_segment_data = time_series_price.iloc[segments[i-1]:segments[i]]\n",
    "                if not prev_segment_data.empty:\n",
    "                    prev_segment_mean = prev_segment_data.mean()\n",
    "                    if not pd.isna(prev_segment_mean):\n",
    "                        price_change = segment_data.mean() - prev_segment_mean\n",
    "                        percent_change = (price_change / prev_segment_mean) * 100 if prev_segment_mean != 0 else float('inf') # Handle division by zero\n",
    "                        print(f\"  Change from previous segment: {price_change:.2f} USD ({percent_change:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n--- Ruptures (Price Mean Shift) Impact ---\")\n",
    "    print(\"Skipping impact analysis for ruptures as change points were not detected or data is not available.\")\n",
    "\n",
    "# Example of quantifying impact for PyMC (Log Return Volatility Shift)\n",
    "if 'trace_volatility' in locals():\n",
    "    print(\"\\n--- PyMC (Log Return Volatility Shift) Impact ---\")\n",
    "    posterior_mu_log_return = trace_volatility.posterior[\"mu_log_return\"].mean().item()\n",
    "    posterior_sigma_1 = trace_volatility.posterior[\"sigma_1\"].mean().item()\n",
    "    posterior_sigma_2 = trace_volatility.posterior[\"sigma_2\"].mean().item()\n",
    "\n",
    "    print(f\"Estimated Mean Log Return (overall): {posterior_mu_log_return:.4f}\")\n",
    "    print(f\"Estimated Std Dev of Log Returns before Change (sigma_1): {posterior_sigma_1:.4f}\")\n",
    "    print(f\"Estimated Std Dev of Log Returns after Change (sigma_2): {posterior_sigma_2:.4f}\")\n",
    "    # Handle division by zero for percentage change\n",
    "    percent_change_volatility = ((posterior_sigma_2 - posterior_sigma_1) / posterior_sigma_1) * 100 if posterior_sigma_1 != 0 else float('inf')\n",
    "    print(f\"Percentage change in volatility: {percent_change_volatility:.2f}%\")\n",
    "else:\n",
    "    print(\"\\n--- PyMC (Log Return Volatility Shift) Impact ---\")\n",
    "    print(\"Skipping impact analysis for PyMC as the Bayesian model was not run successfully.\")\n",
    "\n",
    "\n",
    "print(\"\\nc. Modeling: Build separate models for each segment if the underlying process has truly changed.\")\n",
    "print(\"   This can lead to more accurate forecasts and better understanding of the system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b089e-06be-4a9f-b2fc-3b37602a8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace_volatility, var_names=[\"tau\"], hdi_prob=0.95)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4417e5f-1db4-4fa1-89be-fe5d25b93e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (week100)",
   "language": "python",
   "name": "week100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
